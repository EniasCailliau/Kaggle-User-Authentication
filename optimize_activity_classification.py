import datetime
import os
from datetime import datetime
import xgboost as xgb
from GPyOpt.methods import BayesianOptimization
from sklearn import preprocessing
from sklearn.metrics import roc_auc_score, accuracy_score

import trainer as t
from model_evaluation import visualiser, CustomKFold
from utils import pandaman
import numpy as np
from functools import partial
import matplotlib.pyplot as plt


def print_stats(test_features, train_activity_labels, train_features, train_session_id, train_subject_labels):
    pandaman.print_stats(train_features=train_features, train_activity_labels=train_activity_labels,
                         train_subject_labels=train_subject_labels, train_session_id=train_session_id,
                         test_features=test_features)


def plot_curves(estimator, results_location, train_labels, train_features, train_session_id):
    visualiser.plot_learning_curves(estimator, train_features, train_labels, train_session_id,
                                    results_location)
    visualiser.plot_confusion_matrix(estimator, train_features, train_labels, train_session_id, results_location)


yolo = [{'name': 'n_estimators', 'type': 'discrete', 'domain': (100, 5000, 1)},
        {'name': 'max_depth', 'type': 'discrete', 'domain': (2, 15, 1)},
        {'name': 'min_child_weight', 'type': 'discrete', 'domain': (1, 12, 1)},
        {'name': 'gamma', 'type': 'continuous', 'domain': (0, 1)},
        {'name': 'subsample', 'type': 'continuous', 'domain': (0.6, 1.0)},
        {'name': 'colsample_bytree', 'type': 'continuous', 'domain': (0.6, 1.0)},
        {'name': 'reg_alpha', 'type': 'continuous', 'domain': (1e-5, 100)},
        {'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)},
        {'name': 'n_folds', 'type': 'discrete', 'domain': (2, 2000, 1)},
        ]

activity_to_index = {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5, '7': 6, '12': 7, '13': 8, '16': 9, '17': 10,
                     '24': 11}


def generate_folds(train_features, train_activity_labels, train_sessions):
    folds = []
    for num, (train_index, test_index) in enumerate(CustomKFold.cv(4, train_sessions)):
        train_X, test_X = train_features.iloc[train_index], train_features.iloc[test_index]
        train_Y, test_Y = train_activity_labels.iloc[train_index], train_activity_labels.iloc[test_index]
        xg_train = xgb.DMatrix(train_X, label=train_Y)
        xg_test = xgb.DMatrix(test_X, label=test_Y)
        xg_valid = xgb.DMatrix(test_X)
        test_Y_onehot = preprocessing.label_binarize(test_Y, np.unique(test_Y))
        folds.append((xg_train, xg_test, xg_valid, test_Y, test_Y_onehot))
    return folds


def evaluate(param, folds, numRounds):
    auc_scores, acc_scores = [], []
    print("I am using {} rounds".format(numRounds))
    for i in range(4):
        xg_train, xg_test, xg_valid, test_Y, test_Y_onehot = folds[i]
        watchlist = [(xg_train, 'train'), (xg_test, 'test')]
        bst = xgb.train(param, xg_train, numRounds, watchlist, early_stopping_rounds=50)
        y_scores = bst.predict(xg_valid, output_margin=False, ntree_limit=0)
        auc = roc_auc_score(test_Y_onehot, y_scores, average='macro')
        print("---- Intermediate score auc: {}".format(auc))
        auc_scores.append(auc)

        y_pred = np.argmax(y_scores, axis=1)
        acc = accuracy_score(test_Y, y_pred)
        print("---- Intermediate score acc: {}".format(acc))
        acc_scores.append(acc)
    return np.mean(auc_scores), np.std(auc_scores), np.mean(acc_scores), np.std(acc_scores)


def xgbCv(x, folds):
    fs = np.zeros((x.shape[0], 1))
    for i, params in enumerate(x):
        dict_params = {}
        dict_params['n_estimators'] = int(params[0])
        dict_params['max_depth'] = int(params[1])
        dict_params['min_child_weight'] = int(params[2])
        dict_params['gamma'] = params[3]
        dict_params['subsample'] = params[4]
        dict_params['colsample_bytree'] = params[5]
        dict_params['reg_alpha'] = params[6]
        dict_params['learning_rate'] = params[7]
        dict_params['objective'] = 'multi:softprob'
        dict_params['num_class'] = 12
        dict_params['silent'] = 1
        print(dict_params)
        auc_mean, auc_std, acc_mean, acc_std = evaluate(dict_params, folds, int(params[8]))
        fs[i] = acc_mean
    return fs


def bayesOpt(folds):
    opt = BayesianOptimization(f=partial(xgbCv, folds=folds),
                               domain=yolo,
                               num_cores=8,
                               optimize_restarts=15,
                               acquisition_type='MPI',
                               acquisition_weight=0.1,
                               batch_size=5,
                               maximize=True)

    opt.run_optimization(max_iter=100, eps=0)
    opt.plot_acquisition("acquisition.png")
    opt.plot_convergence("convergence.png")

    print('opt_Y')
    print(opt.Y)
    print('opt_X')
    print(opt.X)
    print("selected:")
    print(np.argmax(opt.Y))
    params = opt.X[np.argmax(opt.Y)]
    dict_params = {}
    dict_params['n_estimators'] = int(params[0])
    dict_params['max_depth'] = int(params[1])
    dict_params['min_child_weight'] = int(params[2])
    dict_params['gamma'] = params[3]
    dict_params['subsample'] = params[4]
    dict_params['colsample_bytree'] = params[5]
    dict_params['reg_alpha'] = params[6]
    dict_params['learning_rate'] = params[7]
    dict_params['objective'] = 'multi:softprob'
    dict_params['num_class'] = 12
    dict_params['silent'] = 1

    print("best params:")
    print(dict_params)

    auc_mean, auc_std, acc_mean, acc_std = evaluate(dict_params, folds, int(params[8]))
    print("I have auc: {} +- {}".format(auc_mean, auc_std))
    print("I have acc: {} +- {}".format(acc_mean, acc_std))


def main():
    # try:
    options = ["EC", "activity", "XGB", "unreduced", "temp"]
    results_location = os.path.join("Results", '/'.join(options) + "/")
    # init trainer
    trainer = t.Trainer("")
    # load data from feature file
    train_features, train_activity_labels, train_subject_labels, train_sessions, test_features = trainer.load_data(
        os.path.join("feature_extraction", '_data_sets/augmented.pkl'), final=False)
    train_activity_labels = train_activity_labels.apply(lambda x: activity_to_index[x])

    print_stats(test_features, train_activity_labels, train_features, train_sessions, train_subject_labels)
    global_start_time = datetime.now()

    # param = {
    #     'nrounds': 100000,
    #     'n_estimators': 200,
    #     'learning_rate': 0.1,
    #     'max_depth': 15,
    #     'min_child_weight': 1,
    #     'subsample': .7,
    #     'colsample_bytree': .7,
    #     'gamma': 0.05,
    #     'scale_pos_weight': 1,
    #     'nthread': 8,
    #     'eta': 0.1
    # }


    print('opt_Y')
    opt_y = [[-0.86581908],
             [-0.92874783],
             [-0.70145431],
             [-0.66459343],
             [-0.85639806],
             [-0.87057532],
             [-0.92911369],
             [-0.89920424],
             [-0.10271655],
             [-0.90423489],
             [-0.90158237],
             [-0.89792372],
             [-0.9014909],
             [-0.10271655],
             [-0.66559956],
             [-0.70044818],
             [-0.92911369],
             [-0.77554194],
             [-0.70136285],
             [-0.86956919],
             [-0.93862618],
             [-0.67483765],
             [-0.92280252],
             [-0.9136559],
             [-0.8811854],
             [-0.88886856],
             [-0.92838196],
             [-0.93597366],
             [-0.77407848],
             [-0.894448],
             [-0.80755511],
             [-0.92197933],
             [-0.81853105],
             [-0.91109485],
             [-0.88859416],
             [-0.69761273],
             [-0.81661026],
             [-0.89728345],
             [-0.9271929],
             [-0.81093936],
             [-0.77837739],
             [-0.93935791],
             [-0.74197384],
             [-0.91813775],
             [-0.84935516],
             [-0.86078844],
             [-0.88813683],
             [-0.92454038],
             [-0.84505625],
             [-0.92682704],
             [-0.81597],
             [-0.92307692],
             [-0.80398793],
             [-0.70209458],
             [-0.87880728],
             [-0.92563798],
             [-0.92344279],
             [-0.76328547],
             [-0.90313729],
             [-0.93213208],
             [-0.87167292],
             [-0.93652245],
             [-0.90322876],
             [-0.90405195],
             [-0.92298546],
             [-0.90158237],
             [-0.93057715],
             [-0.68124028],
             [-0.80444526],
             [-0.68215494],
             [-0.87825848],
             [-0.88969176],
             [-0.93725418],
             [-0.94036404],
             [-0.92600384],
             [-0.93633952],
             [-0.92499771],
             [-0.92572944],
             [-0.93762005],
             [-0.92819903],
             [-0.93204061],
             [-0.92435745],
             [-0.92454038],
             [-0.92920516],
             [-0.92655264],
             [-0.9277417],
             [-0.92106467],
             [-0.93021129],
             [-0.93002835],
             [-0.92673557],
             [-0.92289399],
             [-0.9277417],
             [-0.92938809],
             [-0.9139303],
             [-0.93963231],
             [-0.92536358],
             [-0.93880911],
             [-0.92664411],
             [-0.89353334],
             [-0.8876795],
             [-0.90514955],
             [-0.93981524],
             [-0.89481387],
             [-0.90606421],
             [-0.92426598]]
    print(opt_y)
    opt_x = [[1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 5.06892474e-01,
              6.69057605e-01, 7.10889552e-01, 9.31457248e+01, 5.07765250e-01,
              1.00000000e+00],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 1.58102719e-02,
              7.81308935e-01, 9.01806746e-01, 6.25306950e+01, 1.90452976e-01,
              2.00000000e+03],
             [5.00000000e+03, 1.00000000e+00, 1.00000000e+00, 6.77236787e-01,
              9.94280866e-01, 8.88696350e-01, 6.67548083e+01, 5.26220061e-01,
              1.00000000e+00],
             [1.00000000e+02, 1.00000000e+00, 1.00000000e+00, 3.48606708e-01,
              8.48522164e-01, 7.04731704e-01, 5.84355005e+01, 6.66452717e-01,
              1.00000000e+00],
             [5.00000000e+03, 2.00000000e+00, 1.00000000e+00, 7.62168429e-01,
              8.21288505e-01, 6.14590387e-01, 2.49601446e+01, 9.01016963e-01,
              2.00000000e+00],
             [1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 7.19991845e-01,
              6.89053271e-01, 8.07018966e-01, 9.41323729e+01, 3.00264784e-01,
              1.00000000e+00],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 3.56725592e-01,
              7.95150126e-01, 8.52191135e-01, 5.99655002e+01, 2.21310132e-01,
              2.00000000e+03],
             [5.00000000e+03, 1.50000000e+01, 1.00000000e+00, 1.00000000e+00,
              6.00000000e-01, 6.00000000e-01, 1.00000000e-05, 1.00000000e+00,
              2.00000000e+00],
             [5.00000000e+03,
              1.00000000e+00, 1.20000000e+01, 0.00000000e+00,
              1.00000000e+00, 1.00000000e+00, 1.00000000e-05,
              0.00000000e+00,
              1.00000000e+00],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 2.69944334e-02,
              6.25450628e-01, 9.82322750e-01, 6.11714770e+01, 9.05867800e-01,
              2.00000000e+03],
             [5.00000000e+03,
              1.50000000e+01, 1.00000000e+00, 1.00000000e+00,
              6.00000000e-01, 6.00000000e-01, 4.27459893e+00, 1.00000000e+00,
              2.00000000e+00],
             [5.00000000e+03, 1.50000000e+01, 1.00000000e+00, 1.00000000e+00,
              6.00000000e-01, 6.00000000e-01, 2.23126119e+01, 1.00000000e+00,
              2.00000000e+00],
             [5.00000000e+03,
              1.50000000e+01, 1.00000000e+00, 1.00000000e+00,
              6.00000000e-01, 6.00000000e-01, 3.05491351e+01, 1.00000000e+00,
              2.00000000e+00],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 6.41961767e-01,
              1.00000000e+00, 6.75287945e-01, 5.99208433e+01, 0.00000000e+00,
              2.00000000e+03],
             [1.00000000e+02, 1.00000000e+00, 1.00000000e+00, 7.36801151e-01,
              8.39068771e-01, 6.99897174e-01, 1.19076718e+01, 8.84807248e-01,
              1.00000000e+00],
             [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 3.95133131e-01,
              8.07084017e-01, 9.25142992e-01, 4.80340679e+01, 8.77505723e-01,
              1.00000000e+00],
             [5.00000000e+03,
              2.00000000e+00, 1.00000000e+00, 1.01173469e-01,
              6.34765647e-01, 6.54358016e-01, 2.60070926e+01, 7.71327624e-01,
              2.00000000e+03],
             [5.00000000e+03, 1.00000000e+00, 1.20000000e+01, 1.70933953e-01,
              9.54725625e-01, 6.38829798e-01, 8.34174824e+01, 6.15847325e-01,
              2.00000000e+00],
             [5.00000000e+03, 1.00000000e+00, 1.20000000e+01, 4.07219501e-01,
              9.21629083e-01, 9.36613778e-01, 6.34422399e+01, 3.55427986e-01,
              1.00000000e+00],
             [5.00000000e+03, 1.50000000e+01, 1.00000000e+00, 7.35990172e-01,
              6.18517860e-01, 9.38630843e-01, 9.52186973e+01, 8.04317862e-01,
              1.00000000e+00],
             [1.00000000e+00, 2.00000000e+00, 1.20000000e+01, 5.05302442e-01,
              6.02511499e-01, 8.72454053e-01, 2.50331610e+00, 4.58256197e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.00000000e+00, 1.00000000e+00, 6.67352629e-02,
              6.98141587e-01, 6.23346429e-01, 5.12799470e+00, 3.12108968e-01,
              1.00000000e+00],
             [1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 6.85022467e-01,
              8.72100432e-01, 8.76103542e-01, 2.63389152e+01, 9.20026430e-01,
              2.00000000e+03],
             [1.00000000e+00, 1.50000000e+01, 1.00000000e+00, 9.65681438e-01,
              7.10484581e-01, 8.12860789e-01, 3.85786256e+01, 4.47681175e-01,
              2.00000000e+00],
             [1.00000000e+00, 1.50000000e+01, 1.20000000e+01, 9.83920439e-01,
              9.72860312e-01, 7.74381003e-01, 7.61913259e+01, 7.60517869e-01,
              1.00000000e+00],
             [1.00000000e+00, 1.50000000e+01, 1.00000000e+00, 5.59074976e-01,
              8.27831135e-01, 8.14322274e-01, 6.96216349e+01, 1.81680266e-02,
              2.00000000e+00],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 2.32227992e-01,
              7.51976634e-01, 9.27086416e-01, 4.43799289e+01, 5.07522797e-01,
              2.00000000e+03],
             [5.00000000e+03,
              1.50000000e+01, 1.00000000e+00, 5.33658466e-01,
              7.37904170e-01, 6.02949727e-01, 3.80208305e+01, 2.72489673e-01,
              2.00000000e+03],
             [1.00000000e+00, 1.00000000e+00, 1.00000000e+00, 4.98103388e-01,
              7.26007927e-01, 7.61431852e-01, 3.16087532e-01, 5.93867518e-01,
              2.00000000e+00],
             [1.00000000e+02, 1.50000000e+01, 1.20000000e+01, 3.62353071e-01,
              6.20944397e-01, 9.96110401e-01, 7.22235769e+01, 9.74296167e-01,
              2.00000000e+00],
             [1.00000000e+00, 2.00000000e+00, 1.20000000e+01, 5.75042558e-01,
              8.32691100e-01, 8.47534344e-01, 3.75119706e+01, 2.12572427e-01,
              1.00000000e+00],
             [1.00000000e+02, 1.00000000e+00, 1.00000000e+00, 8.16874648e-01,
              7.18706032e-01, 6.63876688e-01, 7.34085615e+01, 2.59244729e-01,
              2.00000000e+03],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 7.24925399e-02,
              6.68264592e-01, 9.83466877e-01, 8.69410716e+01, 3.15470795e-01,
              1.00000000e+00],
             [1.00000000e+00, 1.00000000e+00, 1.20000000e+01, 7.70597560e-01,
              9.65804465e-01, 9.29452887e-01, 8.52000625e+01, 7.09344087e-01,
              2.00000000e+03],
             [1.00000000e+00, 1.50000000e+01, 1.00000000e+00, 7.72622506e-01,
              8.02709387e-01, 8.17915469e-01, 8.32997902e+01, 9.72972619e-02,
              2.00000000e+00],
             [1.00000000e+00, 1.00000000e+00, 1.20000000e+01, 2.96200439e-01,
              6.29533180e-01, 7.90206598e-01, 3.13136163e+01, 2.63719888e-01,
              1.00000000e+00],
             [1.00000000e+00, 2.00000000e+00, 1.20000000e+01, 3.53189638e-01,
              6.29726121e-01, 7.80963227e-01, 9.89114310e+01, 3.05800271e-01,
              1.00000000e+00],
             [1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 3.36501059e-01,
              8.01406901e-01, 6.52260140e-01, 4.30220823e+00, 6.70934579e-02,
              1.00000000e+00],
             [1.00000000e+00, 2.00000000e+00, 1.00000000e+00, 5.96146579e-02,
              7.28720631e-01, 7.05959164e-01, 8.17951043e+01, 1.52421681e-01,
              2.00000000e+03],
             [1.00000000e+00, 2.00000000e+00, 1.00000000e+00, 4.28310603e-01,
              7.70048389e-01, 9.73626798e-01, 4.33690158e+01, 5.47957025e-01,
              1.00000000e+00],
             [1.00000000e+02, 1.00000000e+00, 1.20000000e+01, 9.52147541e-01,
              6.35301251e-01, 7.63651579e-01, 7.43197067e+01, 6.76324383e-01,
              2.00000000e+00],
             [1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 5.74531485e-01,
              7.07954504e-01, 6.95333254e-01, 7.80322292e+00, 2.20220151e-01,
              2.00000000e+03],
             [1.00000000e+00, 1.00000000e+00, 1.20000000e+01, 8.43195895e-01,
              8.85378188e-01, 9.64561113e-01, 6.00050418e+01, 1.73393599e-01,
              2.00000000e+00],
             [1.00000000e+00, 2.00000000e+00, 1.20000000e+01, 9.64308116e-01,
              8.61393663e-01, 9.39263599e-01, 9.05337119e+01, 4.56966044e-01,
              2.00000000e+03],
             [5.00000000e+03, 2.00000000e+00, 1.00000000e+00, 2.38586878e-01,
              8.83982467e-01, 7.92254061e-01, 4.64664101e+01, 4.14231735e-01,
              2.00000000e+00],
             [5.00000000e+03,
              2.00000000e+00, 1.20000000e+01, 6.82596338e-01,
              6.30555020e-01, 7.37265036e-01, 5.72674847e+01, 7.54690627e-01,
              2.00000000e+00],
             [1.00000000e+00, 1.50000000e+01, 1.00000000e+00, 7.00059364e-01,
              6.33098152e-01, 8.21650449e-01, 1.36022121e+01, 5.52222895e-01,
              1.00000000e+00],
             [1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 2.82479902e-01,
              6.15527372e-01, 7.93255980e-01, 3.79462941e+01, 9.01470943e-01,
              2.00000000e+03],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 7.74682162e-01,
              7.22149616e-01, 8.31488829e-01, 2.17404101e+01, 5.27847826e-01,
              2.00000000e+00],
             [1.00000000e+02, 2.00000000e+00, 1.00000000e+00, 3.02632948e-01,
              9.74697408e-01, 6.31428156e-01, 9.40114701e+01, 1.43476958e-01,
              2.00000000e+03],
             [1.00000000e+00, 2.00000000e+00, 1.00000000e+00, 2.69237019e-01,
              6.06413768e-01, 9.49988580e-01, 9.04774225e+01, 1.03786065e-02,
              1.00000000e+00],
             [5.00000000e+03,
              1.50000000e+01, 1.00000000e+00, 8.07440860e-01,
              6.63786387e-01, 8.95678565e-01, 1.24600929e+01, 8.63486252e-01,
              2.00000000e+03],
             [1.00000000e+00, 2.00000000e+00, 1.00000000e+00, 3.57804766e-01,
              9.63717679e-01, 7.73500517e-01, 6.86544372e+01, 4.64421221e-01,
              1.00000000e+00],
             [1.00000000e+02, 1.00000000e+00, 1.00000000e+00, 8.51350656e-01,
              7.48398875e-01, 9.41451319e-01, 5.07140612e+01, 3.08603398e-01,
              1.00000000e+00],
             [1.00000000e+02, 1.50000000e+01, 1.20000000e+01, 4.22001553e-01,
              8.57233261e-01, 9.91289547e-01, 5.10216133e+01, 3.34696180e-01,
              1.00000000e+00],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 5.37616436e-01,
              9.03552005e-01, 9.12029038e-01, 4.70999981e+01, 6.59601491e-01,
              2.00000000e+03],
             [5.00000000e+03, 1.50000000e+01, 1.00000000e+00, 2.88581830e-01,
              9.77042446e-01, 9.86605021e-01, 2.23211901e+01, 9.25350969e-01,
              2.00000000e+03],
             [5.00000000e+03,
              1.00000000e+00, 1.00000000e+00, 7.76275018e-01,
              8.95030388e-01, 6.52134485e-01, 9.27976923e+01, 3.01946625e-01,
              2.00000000e+00],
             [5.00000000e+03, 1.50000000e+01, 1.20000000e+01, 6.09216112e-01,
              9.24074627e-01, 8.18721210e-01, 3.73555285e+01, 1.71178059e-01,
              2.00000000e+00],
             [1.00000000e+02, 1.00000000e+00, 1.00000000e+00, 5.25181374e-01,
              9.08564796e-01, 9.31800577e-01, 4.07545918e+01, 1.57381709e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 4.92786432e-01,
              9.85588989e-01, 6.58776640e-01, 8.04306692e+01, 3.94787216e-01,
              1.00000000e+00],
             [5.00000000e+03,
              1.50000000e+01, 1.00000000e+00, 4.44287310e-01,
              8.59521387e-01, 6.88823579e-01, 3.52392739e+01, 3.51723460e-01,
              2.00000000e+03],
             [1.00000000e+00, 1.50000000e+01, 1.00000000e+00, 8.80549794e-01,
              9.25701682e-01, 7.01647897e-01, 7.63216581e+01, 9.74980989e-01,
              2.00000000e+00],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 3.63249569e-01,
              6.98258549e-01, 7.47415518e-01, 8.56863046e+01, 8.92588767e-01,
              2.00000000e+03],
             [1.00000000e+02, 2.00000000e+00, 1.00000000e+00, 8.93517820e-01,
              6.09369420e-01, 7.33771375e-01, 5.96360689e+01, 4.75181966e-01,
              2.00000000e+03],
             [5.00000000e+03,
              1.50000000e+01, 1.20000000e+01, 3.00490053e-01,
              9.45226767e-01, 6.61398220e-01, 8.46075999e+01, 8.93267561e-01,
              2.00000000e+00],
             [1.00000000e+02, 1.00000000e+00, 1.20000000e+01, 5.60760819e-02,
              8.34197229e-01, 6.77773257e-01, 2.80594619e+01, 6.05593611e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.00000000e+00, 1.00000000e+00, 3.83638448e-01,
              6.10958786e-01, 7.42961073e-01, 2.73959961e+01, 9.21055751e-02,
              1.00000000e+00],
             [5.00000000e+03, 2.00000000e+00, 1.00000000e+00, 1.38186092e-02,
              8.70131013e-01, 6.69418458e-01, 9.08608140e+01, 8.71922976e-01,
              1.00000000e+00],
             [5.00000000e+03,
              1.00000000e+00, 1.00000000e+00, 4.15597158e-01,
              8.53782457e-01, 7.26122314e-01, 6.61654149e+01, 9.59152175e-01,
              1.00000000e+00],
             [1.00000000e+02, 1.50000000e+01, 1.20000000e+01, 5.70746975e-01,
              9.30970244e-01, 9.29057727e-01, 6.26414021e+01, 5.57911816e-01,
              1.00000000e+00],
             [1.00000000e+00, 1.50000000e+01, 1.00000000e+00, 7.10249137e-01,
              9.52819879e-01, 9.91123850e-01, 8.33839554e+01, 3.94191039e-02,
              2.00000000e+00],
             [5.00000000e+03, 1.50000000e+01, 1.00000000e+00, 5.25444500e-01,
              7.36339565e-01, 6.04159002e-01, 3.80393298e+01, 2.83344775e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 5.80967776e-01,
              7.10263681e-01, 6.96757435e-01, 7.79281569e+00, 2.21324230e-01,
              2.00000000e+03],
             [1.00000000e+00, 2.00000000e+00, 1.00000000e+00, 2.03263711e-02,
              7.24325185e-01, 6.81897855e-01, 8.19080430e+01, 1.52917805e-01,
              2.00000000e+03],
             [5.00000000e+03,
              1.50000000e+01, 1.00000000e+00, 4.06819726e-01,
              8.68312819e-01, 6.79792409e-01, 3.51821117e+01, 3.40492146e-01,
              2.00000000e+03],
             [1.00000000e+02, 2.00000000e+00, 1.00000000e+00, 3.00307389e-01,
              8.78963465e-01, 6.40264329e-01, 9.40218163e+01, 2.23416363e-01,
              2.00000000e+03],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 4.40501155e-01,
              8.93716971e-01, 8.96225030e-01, 4.70871933e+01, 5.73098401e-01,
              2.00000000e+03],
             [1.00000000e+00, 2.00000000e+00, 1.20000000e+01, 5.11932334e-01,
              6.00007243e-01, 8.76176334e-01, 2.55757017e+00, 4.60249558e-01,
              2.00000000e+03],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 1.66926939e-02,
              7.69372432e-01, 9.07965206e-01, 6.24267626e+01, 2.45249449e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.00000000e+00, 1.00000000e+00, 4.77333486e-01,
              8.78655407e-01, 9.31097348e-01, 4.07498363e+01, 2.40312731e-01,
              2.00000000e+03],
             [5.00000000e+03,
              2.00000000e+00, 1.00000000e+00, 1.23043195e-01,
              6.00048674e-01, 6.12355626e-01, 2.59797793e+01, 8.69611210e-01,
              2.00000000e+03],
             [5.00000000e+03, 1.50000000e+01, 1.00000000e+00, 3.31390715e-01,
              9.18994395e-01, 9.31633138e-01, 2.22092094e+01, 8.24387123e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.00000000e+00, 1.20000000e+01, 1.26711955e-01,
              8.42348695e-01, 6.74819183e-01, 2.81873489e+01, 6.35123855e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 3.23410182e-01,
              6.46266931e-01, 7.99016462e-01, 3.80677453e+01, 7.60786024e-01,
              2.00000000e+03],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 9.48593367e-02,
              7.55191490e-01, 9.73366092e-01, 4.43161438e+01, 5.61086461e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.00000000e+00, 1.00000000e+00, 6.46102856e-01,
              6.80497944e-01, 6.48914936e-01, 7.33937509e+01, 3.77625413e-01,
              2.00000000e+03],
             [1.00000000e+02, 2.00000000e+00, 1.20000000e+01, 3.20604202e-01,
              7.69213674e-01, 8.74588300e-01, 5.99712360e+01, 2.49365232e-01,
              2.00000000e+03],
             [1.00000000e+00, 2.00000000e+00, 1.20000000e+01, 6.44475246e-01,
              6.47365622e-01, 7.89032745e-01, 2.65969745e+00, 8.78770035e-01,
              2.00000000e+03],
             [1.00000000e+02, 2.00000000e+00, 1.00000000e+00, 8.99539112e-01,
              7.91102193e-01, 8.23906290e-01, 5.96480236e+01, 4.16629836e-01,
              2.00000000e+03],
             [1.00000000e+02, 2.00000000e+00, 1.00000000e+00, 7.36139757e-01,
              7.15383610e-01, 8.28110286e-01, 5.96501826e+01, 4.62743113e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.00000000e+00, 1.20000000e+01, 1.78110366e-01,
              7.94541756e-01, 7.47136940e-01, 2.80559279e+01, 6.64702312e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.00000000e+00, 1.00000000e+00, 5.25949565e-01,
              6.61558530e-01, 7.36577132e-01, 4.07335300e+01, 3.72281027e-01,
              2.00000000e+03],
             [1.00000000e+00, 1.50000000e+01, 1.00000000e+00, 9.07052849e-01,
              9.47630247e-01, 7.01583343e-01, 3.87822206e+01, 7.28566593e-01,
              2.00000000e+00],
             [5.00000000e+03, 1.50000000e+01, 1.00000000e+00, 1.81493258e-01,
              7.04865758e-01, 8.05312672e-01, 3.78698304e+01, 1.91074622e-01,
              2.00000000e+03],
             [5.00000000e+03,
              1.50000000e+01, 1.00000000e+00, 1.00000000e+00,
              6.00000000e-01, 1.00000000e+00, 1.19719024e+01, 8.27282458e-01,
              2.00000000e+03],
             [1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 6.84845496e-01,
              6.79550052e-01, 8.65847233e-01, 2.59388583e+01, 1.52155356e-01,
              2.00000000e+03],
             [1.00000000e+00, 2.00000000e+00, 1.20000000e+01, 2.75147368e-01,
              8.72945542e-01, 9.48003529e-01, 8.50394685e+01, 2.38369661e-01,
              2.00000000e+03],
             [1.00000000e+00, 1.50000000e+01, 1.00000000e+00, 7.44305917e-01,
              9.77837959e-01, 7.80335394e-01, 1.32447606e+01, 9.45708700e-01,
              1.00000000e+00],
             [5.00000000e+03,
              1.50000000e+01, 1.00000000e+00, 6.48336699e-01,
              6.27647657e-01, 7.99826069e-01, 4.47097314e+00, 4.65164562e-01,
              1.00000000e+00],
             [1.00000000e+00, 1.50000000e+01, 1.00000000e+00, 6.22776335e-01,
              7.74942786e-01, 8.64080582e-01, 7.70083014e+01, 7.36930948e-01,
              2.00000000e+00],
             [1.00000000e+02, 1.50000000e+01, 1.00000000e+00, 5.42515578e-01,
              7.51924023e-01, 8.83629113e-01, 2.53554760e+01, 1.22689495e-01,
              2.00000000e+03],
             [1.00000000e+00, 1.50000000e+01, 1.00000000e+00, 3.10701459e-01,
              8.14629805e-01, 8.32947736e-01, 8.43294098e+01, 1.85410851e-01,
              2.00000000e+00],
             [1.00000000e+00, 2.00000000e+00, 1.00000000e+00, 2.67567413e-01,
              7.41057791e-01, 6.60014447e-01, 8.23019134e+01, 7.63142892e-01,
              2.00000000e+03],
             [1.00000000e+00, 2.00000000e+00, 1.20000000e+01, 9.48047131e-01,
              7.75854847e-01, 7.53305634e-01, 8.98994454e+01, 1.07052361e-01,
              2.00000000e+03]]
    print('opt_X')
    print(opt_x)
    print("selected:")
    print(np.argmin(opt_y))
    print(opt_y[np.argmin(opt_y)])
    params = opt_x[np.argmin(opt_y)]
    dict_params = {}
    dict_params['n_estimators'] = int(params[0])
    dict_params['max_depth'] = int(params[1])
    dict_params['min_child_weight'] = int(params[2])
    dict_params['gamma'] = params[3]
    dict_params['subsample'] = params[4]
    dict_params['colsample_bytree'] = params[5]
    dict_params['reg_alpha'] = params[6]
    dict_params['learning_rate'] = params[7]
    dict_params['objective'] = 'multi:softprob'
    # dict_params['num_class'] = 12
    dict_params['silent'] = 1

    print("best params:")
    print(dict_params)

    folds = generate_folds(train_features, train_activity_labels, train_sessions)

    # auc_mean, auc_std, acc_mean, acc_std = evaluate(dict_params, folds, int(params[8]))
    # print("I have auc: {} +- {}".format(auc_mean, auc_std))
    # print("I have acc: {} +- {}".format(acc_mean, acc_std))


    estimator = xgb.XGBClassifier(**dict_params)
    trainer.evaluate(estimator, train_features, train_activity_labels, train_sessions, accuracy=True)
    visualiser.plot_acc_learning_curve(estimator, train_features, train_activity_labels, train_sessions, "test/")

    # print("starting with bayesian optimisation:")
    # bayesOpt(folds)

    time_elapsed = datetime.now() - global_start_time

    print('Time elpased (hh:mm:ss.ms) {}'.format(time_elapsed))

    os.system("say Your program has finished")
    os.system("say Your program has finished")
    os.system("say Your program has finished")


# except Exception as e:
#     print(e)
#     send_notification("Exception occurred", "{}".format(e))


if __name__ == '__main__':
    main()
